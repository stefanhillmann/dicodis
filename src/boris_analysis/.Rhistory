0, 0)
d$response <- 0
d$response[which(d$true_class == 'positive')] <- 1
plot(roc(d$response,
abs(d$positive_class_distance),
percent=FALSE))
rd <- dbGetQuery(cross_validation,
"documents_result",
'{"classifier_name": "cosine", "frequency_threshold": 1, "n_gram_size": 1, "criteria": "long_interactions", "smoothing_value": 0.5}',
0, 0)
rd$response <- 0
rd$response[which(rd$true_class == 'positive')] <- 1
plot(roc(rd$response,
abs(rd$positive_class_distance),
percent=FALSE))
roc <- roc(d$response,
abs(d$positive_class_distance),
percent=FALSE)
plot(roc)
auc(roc)
roc$percent
roc$direction
roc$cases
rd <- dbGetQuery(cross_validation,
"documents_result",
'{"classifier_name": "cosine", "frequency_threshold": 1, "n_gram_size": 1, "criteria": "long_interactions", "smoothing_value": 0.5}',
0, 0)
rd$response <- 1
rd$response[which(rd$true_class == 'positive')] <- 0
plot(roc(rd$response,
abs(rd$positive_class_distance),
percent=FALSE))
rd <- dbGetQuery(cross_validation,
"documents_result",
'{"classifier_name": "cosine", "frequency_threshold": 1, "n_gram_size": 1, "criteria": "long_interactions", "smoothing_value": 0.5}',
0, 0)
rd$response <- 0
rd$response[which(rd$true_class == 'positive')] <- 1
plot(roc(rd$response,
abs(rd$positive_class_distance),
percent=FALSE))
d$score <- 0
names(d)
d$score[which(d$positive_class_distance > d$negative_class_distance)] <- d$positive_class_distance - d$negative_class_distance
which(d$positive_class_distance > d$negative_class_distance)
d$score <- d$positive_class_distance - d$negative_class_distance
d$score
d$score[which(d$score < 0)] <- 0
d$score
roc <- roc(d$response,
abs(d$positive_class_distance),
percent=FALSE)
auc(roc)
plot(roc)
roc <- roc(d$response,
abs(d$score),
percent=FALSE)
auc(roc)
plot(roc)
rd <- dbGetQuery(cross_validation,
"documents_result",
'{"classifier_name": "cosine", "frequency_threshold": 1, "n_gram_size": 1, "criteria": "long_interactions", "smoothing_value": 0.5}',
0, 0)
rd$response <- 0
rd$response[which(rd$true_class == 'positive')] <- 1
rd$score <- rd$positive_class_distance - rd$negative_class_distance
rd$score[which(rd$score < 0)] <- 0
plot(roc(rd$response,
abs(rd$score),
percent=FALSE))
methods("auc")
methods("auc")
library(pROC)
methods("auc")
getAnywhere("auc.default")
getAnywhere("auc.smooth.roc")
methods("auc")
methods("auc.formula")
getAnywhere("auc.formula")
source('~/.active-rstudio-document', echo=TRUE)
scenario_a <- dbGetQuery(cross_validation, "documents_result", '{classifier_name: 'rank order', frequency_threshold: 1, n_gram_size: 8, criteria: 'word_accuracy_60', smoothing_value: 0.25}', 0, 0)
scenario_a <- dbGetQuery(cross_validation, "documents_result", "{classifier_name: 'rank order', frequency_threshold: 1, n_gram_size: 8, criteria: 'word_accuracy_60', smoothing_value: 0.25}", 0, 0)
scenario_a <- dbGetQuery(cross_validation, "documents_result", "{classifier_name: 'rank order', frequency_threshold: 1, n_gram_size: 8, criteria: 'word_accuracy_60', smoothing_value: 0.25}", 0, 0)
ggplot(scenario_a, aes(x=positive_class_distance, fill=true_class)) +
geom_histogram(binwidth=.5, alpha=.5, position="identity")
ggplot(scenario_a, aes(x=positive_class_distance, fill=true_class)) +
geom_histogram(alpha=.5, position="identity")
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
geom_histogram(alpha=.5, position="identity")
scenario_a <- dbGetQuery(cross_validation, "documents_result", "{classifier_name: 'rank order', frequency_threshold: 1, n_gram_size: 8, criteria: 'word_accuracy_60', smoothing_value: 0.25}", 0, 0)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
geom_histogram(alpha=.5, position="identity") +
geom_density(alpha=.3)
scenario_a <- dbGetQuery(cross_validation, "documents_result", "{classifier_name: 'rank order', frequency_threshold: 1, n_gram_size: 8, criteria: 'word_accuracy_60', smoothing_value: 0.25}", 0, 0)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
geom_histogram(alpha=.5, aes=(y=..density..)) +
geom_density(alpha=.3)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=cond)) + geom_density(alpha=.3)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) + geom_density(alpha=.3)
scenario_a <- dbGetQuery(cross_validation, "documents_result", "{classifier_name: 'rank order', frequency_threshold: 1, n_gram_size: 8, criteria: 'word_accuracy_60', smoothing_value: 0.25}", 0, 0)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
geom_histogram(alpha=.5, aes=(y=..density..)) +
geom_density(alpha=.3)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
geom_histogram(alpha=.5, position="identity") +
geom_density(alpha=.3)
scenario_a <- dbGetQuery(cross_validation, "documents_result", "{classifier_name: 'rank order', frequency_threshold: 1, n_gram_size: 8, criteria: 'word_accuracy_60', smoothing_value: 0.25}", 0, 0)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
geom_histogram(alpha=.5, position="identity") +
geom_density(alpha=.3)
scenario_a <- dbGetQuery(cross_validation, "documents_result", "{classifier_name: 'rank order', frequency_threshold: 1, n_gram_size: 8, criteria: 'word_accuracy_60', smoothing_value: 0.25}", 0, 0)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
#geom_histogram(alpha=.5, position="identity") +
geom_dotplot()
scenario_a <- dbGetQuery(cross_validation, "documents_result", "{classifier_name: 'rank order', frequency_threshold: 1, n_gram_size: 8, criteria: 'word_accuracy_60', smoothing_value: 0.25}", 0, 0)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
#geom_histogram(alpha=.5, position="identity") +
geom_dotplot(dotsize = 0.75)
auc_is_1 <- dbGetQuery(cross_validation, "documents_result", "{auc: 1}", 0, 0)
auc_is_1 <- dbGetQuery(cross_validation, "performance", "{auc: 1}", 0, 0)
p <- dbGetQuery(cross_validation, "performance", "{auc: 1}", 0, 0)
# get all performance entries with auc = 1
p <- dbGetQuery(cross_validation, "performance", "{auc: 1}", 0, 0)
# loop over the entries and plot the related data from document_results
for (i in 1:nrow(auc_is_1)) {
q <- sprintf("{classifier_name: '%s', frequency_threshold: %d, n_gram_size: %d, criteria: '%s', smoothing_value: %f}",
p$classifier_name, p$frequency_threshold, p$n_gram_size, p$criteria, p$smoothing_value)
r <- dbGetQuery(cross_validation, "documents_result", q, 0, 0)
print(r)
}
#auc_is_1 <- dbGetQuery(cross_validation, "documents_result", "{auc: 1}", 0, 0)
#ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
#geom_histogram(alpha=.5, position="identity") +
#  geom_dotplot(dotsize = 0.75)
# get all performance entries with auc = 1
p <- dbGetQuery(cross_validation, "performance", "{auc: 1}", 0, 0)
# loop over the entries and plot the related data from document_results
for (i in 1:nrow(auc_is_1)) {
q <- sprintf("{classifier_name: '%s', frequency_threshold: %d, n_gram_size: %d, criteria: '%s', smoothing_value: %f}",
p$classifier_name, p$frequency_threshold, p$n_gram_size, p$criteria, p$smoothing_value)
print(q)
r <- dbGetQuery(cross_validation, "documents_result", q, 0, 0)
print(r)
}
#auc_is_1 <- dbGetQuery(cross_validation, "documents_result", "{auc: 1}", 0, 0)
#ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
#geom_histogram(alpha=.5, position="identity") +
#  geom_dotplot(dotsize = 0.75)
r <- dbGetQuery(cross_validation, "documents_result", q, 0, 0)
p <- dbGetQuery(cross_validation, "performance", "{auc: 1}", 0, 0)
r <- dbGetQuery(cross_validation, "documents_result", "q", 0, 0)
str(q)
as.character(q)
as.character(q)
?sprintf
# get all performance entries with auc = 1
p <- dbGetQuery(cross_validation, "performance", "{auc: 1}", 0, 0)
# loop over the entries and plot the related data from document_results
for (i in 1:nrow(auc_is_1)) {
p_row <- p[i,]
q <- sprintf("{classifier_name: '%s', frequency_threshold: %d, n_gram_size: %d, criteria: '%s', smoothing_value: %f}",
p_row$classifier_name, p_row$frequency_threshold, p_row$n_gram_size, p_row$criteria, p_row$smoothing_value)
print(q)
r <- dbGetQuery(cross_validation, "documents_result", "q", 0, 0)
print(r)
}
#auc_is_1 <- dbGetQuery(cross_validation, "documents_result", "{auc: 1}", 0, 0)
#ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
#geom_histogram(alpha=.5, position="identity") +
#  geom_dotplot(dotsize = 0.75)
# get all performance entries with auc = 1
p <- dbGetQuery(cross_validation, "performance", "{auc: 1}", 0, 0)
# loop over the entries and plot the related data from document_results
for (i in 1:nrow(auc_is_1)) {
p_row <- p[i,]
q <- sprintf("{classifier_name: '%s', frequency_threshold: %d, n_gram_size: %d, criteria: '%s', smoothing_value: %f}",
p_row$classifier_name, p_row$frequency_threshold, p_row$n_gram_size, p_row$criteria, p_row$smoothing_value)
print(q)
r <- dbGetQuery(cross_validation, "documents_result", q, 0, 0)
print(r)
}
#auc_is_1 <- dbGetQuery(cross_validation, "documents_result", "{auc: 1}", 0, 0)
#ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
#geom_histogram(alpha=.5, position="identity") +
#  geom_dotplot(dotsize = 0.75)
# get all performance entries with auc = 1
p <- dbGetQuery(cross_validation, "performance", "{auc: 1}", 0, 0)
# loop over the entries and plot the related data from document_results
for (i in 1:nrow(auc_is_1)) {
p_row <- p[i,]
q <- sprintf("{classifier_name: '%s', frequency_threshold: %d, n_gram_size: %d, criteria: '%s', smoothing_value: %f}",
p_row$classifier_name, p_row$frequency_threshold, p_row$n_gram_size, p_row$criteria, p_row$smoothing_value)
r <- dbGetQuery(cross_validation, "documents_result", q, 0, 0)
ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
# geom_histogram(alpha=.5, position="identity") +
geom_dotplot(dotsize = 0.75)
}
#auc_is_1 <- dbGetQuery(cross_validation, "documents_result", "{auc: 1}", 0, 0)
#ggplot(scenario_a, aes(x=positive_class_distance-negative_class_distance, fill=true_class)) +
#geom_histogram(alpha=.5, position="identity") +
#  geom_dotplot(dotsize = 0.75)
p <- dbGetQuery(cross_validation, "performance", "{auc: {$gt: 0.8, $lt: 0.9}}", 0, 0)
p <- dbGetQuery(cross_validation, "performance", "{auc: {$gt: 0.8, $lt: 0.82}}", 0, 0)
p <- dbGetQuery(cross_validation, "performance", "{auc: {$gt: 0.8, $lt: 0.81}}", 0, 0)
?print
85+90+80+75
330/4
85*0.35 + 90*0.2 + 80*0.25 + 75*0.2
22*0.35 + 75*0.2 + 50*0.25 + 75*0.2
?sprintf
sprintf("%f", pi)
sprintf("%.3f", pi)
sprintf("%1.0f", pi)
sprintf("%5.1f", pi)
sprintf("%05.1f", pi)
sprintf("%+f", pi)
sprintf("% f", pi)
sprintf("%-10f", pi) # left justified
sprintf("%e", pi)
sprintf("%E", pi)
sprintf("%g", pi)
sprintf("%g",   1e6 * pi) # -> exponential
sprintf("%.9g", 1e6 * pi) # -> "fixed"
sprintf("%G", 1e-6 * pi)
unique(performance$classifier_name)
cutted <- performance[performance$criteria=='juged_bad',]
cutted <- performance[performance$criteria=='juged_bad',]
cutted <- rbind(cutted, performance[performance$criteria=='short_interactions',])
cutted <- performance[performance$criteria=='juged_bad',]
cutted <- rbind(cutted, performance[performance$criteria=='short_interactions',])
cutted <- rbind(cutted, performance[performance$criteria=='real',])
cutted <- rbind(cutted, performance[performance$criteria=='task_failed',])
cutted <- rbind(cutted, performance[performance$criteria=='word_accuracy_100',])
cutted <- rbind(cutted, performance[performance$criteria=='simulation_quality_best',])
cutted <- rbind(cutted, performance[performance$criteria=='real_vs_simulated_worst',])
p <- cutted[which(cutted$auc == 1),]
names(cutted)
ggplot(cutted, aes(x=criteria, y=auc, color=classifier_name)) + geom_point()
cutted$criteria_name <- 'NA'
cutted[which(cutted$criteria == 'short_interactions'),]$criteria_name <- 'dialogue length'
View(cutted)
cutted <- performance[performance$criteria=='juged_bad',]
cutted <- rbind(cutted, performance[performance$criteria=='short_interactions',])
cutted <- rbind(cutted, performance[performance$criteria=='real',])
cutted <- rbind(cutted, performance[performance$criteria=='task_failed',])
cutted <- rbind(cutted, performance[performance$criteria=='word_accuracy_100',])
cutted <- rbind(cutted, performance[performance$criteria=='simulation_quality_best',])
cutted <- rbind(cutted, performance[performance$criteria=='real_vs_simulated_worst',])
# Set names for criteria
cutted$criteria_name <- 'NA'
cutted[which(cutted$criteria == 'juged_bad'),]$criteria_name <- 'user jugedment'
cutted[which(cutted$criteria == 'short_interactions'),]$criteria_name <- 'dialogue length'
cutted[which(cutted$criteria == 'real'),]$criteria_name <- 'real vs simulated'
cutted[which(cutted$criteria == 'task_failed'),]$criteria_name <- 'task success'
cutted[which(cutted$criteria == 'word_accuracy_100'),]$criteria_name <- 'word accuracy'
cutted[which(cutted$criteria == 'simulation_quality_best'),]$criteria_name <- 'real vs. simulated (good)'
cutted[which(cutted$criteria == 'real_vs_simulated_worst'),]$criteria_name <- 'real vs. simulated (bad)'
cutted[which(cutted$criteria_name == 'NA'),]
nrow(cutted[which(cutted$criteria_name == 'NA'),])
nrow(cutted[which(cutted$criteria_name != 'NA'),])
ggplot(cutted[which(cutted$classifier_name == 'rank order')], aes(x=criteria, y=auc, color=n_gram_size)) +
geom_point() +
theme(axis.text.x = element_text(angle=90))
ggplot(cutted[which(cutted$classifier_name == 'rank order'),], aes(x=criteria, y=auc, color=n_gram_size)) +
geom_point() +
theme(axis.text.x = element_text(angle=90))
levels(cutted$n_gram_size)
as.factor(cutted$n_gram_size)
factor(cutted$n_gram_size)
levels(as.factor(cutted$n_gram_size))
ggplot(cutted[which(cutted$classifier_name == 'rank order'),], aes(x=criteria, y=auc, color=levels(as.factor(n_gram_size)))) +
geom_point() +
theme(axis.text.x = element_text(angle=90))
levels(as.factor(n_gram_size))
levels(as.factor(cutted$n_gram_size))
ggplot(cutted[which(cutted$classifier_name == 'rank order'),], aes(x=criteria, y=auc, color=factor(n_gram_size))) +
geom_point() +
theme(axis.text.x = element_text(angle=90))
ggplot(cutted[which(cutted$classifier_name == 'rank order'),], aes(x=criteria_name, y=auc,  shape=factor(n_gram_size))) +
geom_point() +
theme(axis.text.x = element_text(angle=90))
ggplot(cutted[which(cutted$classifier_name == 'rank order'),], aes(x=criteria_name, y=auc, color=factor(n_gram_size))) +
geom_point() +
theme(axis.text.x = element_text(angle=90))
ggplot(cutted[which(cutted$classifier_name == 'rank order'),], aes(x=frequency_threshold, y=auc, color=factor(n_gram_size))) +
geom_point() +
theme(axis.text.x = element_text(angle=90))
ggplot(cutted[which(cutted$classifier_name == 'rank order'),], aes(x=criteria_name, y=auc, color=factor(frequency_threshold))) +
geom_point() +
theme(axis.text.x = element_text(angle=90))
ggplot(cutted[which(cutted$classifier_name == 'rank order'),], aes(x=criteria_name, y=auc, color=factor(smoothing_value))) +
geom_point() +
theme(axis.text.x = element_text(angle=90))
setwd("~/git/DialogueClassifying/src/boris_analysis")
install.packages("rmarkdown")
library(RMongo)
library(ggplot2)
library(plyr)
library(reshape2)
library(knitr)
cross_validation <- mongoDbConnect("classification_cross_validation", "localhost", 27017)
performance <- dbGetQuery(cross_validation, "performance", '{}', 0, 0)
cutted <- performance[performance$criteria=='juged_bad',]
cutted[which(cutted$classifier_name == "rank order"), ]
8*3*2
8*3*2*7
cutted <- performance[performance$criteria=='juged_bad',]
cutted <- rbind(cutted, performance[performance$criteria=='short_interactions',])
cutted <- rbind(cutted, performance[performance$criteria=='real',])
cutted <- rbind(cutted, performance[performance$criteria=='task_failed',])
cutted <- rbind(cutted, performance[performance$criteria=='word_accuracy_100',])
cutted <- rbind(cutted, performance[performance$criteria=='simulation_quality_best',])
cutted <- rbind(cutted, performance[performance$criteria=='real_vs_simulated_worst',])
# for rank order we need only one case for smoothing value, as the smoothing value is
# not used for rank order
cutted[which(cutted$classifier_name == "rank order"), ]
head(cutted)
8*2*3
8*2*3*4
8*2*3*4*7
8*2*3*7
cutted[which(cutted$classifier_name == "rank order"), ]
cutted[which(cutted$classifier_name == "rank order"), ]
length(cutted[which(cutted$classifier_name == "rank order"), ])
nrow(cutted[which(cutted$classifier_name == "rank order"), ])
nrow(cutted[which(cutted$classifier_name == "rank order" & cutted$smoothing_value == 0.05), ])
nrow(cutted[which(cutted$classifier_name == "rank order" & cutted$smoothing_value in c(0.05)), ])
nrow(cutted[which(cutted$classifier_name == "rank order" & cutted$smoothing_value %in% c(0.05)), ])
nrow(cutted[which(cutted$classifier_name == "rank order" & cutted$smoothing_value %in% c(0.05, 0.25)), ])
nrow(cutted[!which(cutted$classifier_name == "rank order" & cutted$smoothing_value %in% c(0.05, 0.25)), ])
nrow(cutted[which( !(cutted$classifier_name == "rank order" & cutted$smoothing_value %in% c(0.05, 0.25)) ), ])
336/3
1344 - 112
nrow(cutted[which( !(cutted$classifier_name == "rank order" & cutted$smoothing_value %in% c(0.05, 0.25)) ), ])
1344-1120
1344 - 224
#   juged.data <- pr.complete[which(pr.complete$criteria %in% c('juged_bad', 'juged_good')), ]
#   interact_length.data <- pr.complete[which(pr.complete$criteria %in% c('short_interactions', 'long_interactions')), ]
#   real_simulated.data <- pr.complete[which(pr.complete$criteria %in% c('real', 'simulated')), ]
#   success.data <- pr.complete[which(pr.complete$criteria %in% c('task_failed', 'task_successful')), ]
#   word_accuracy.data <- pr.complete[which(pr.complete$criteria %in% c('word_accuracy_100', 'word_accuracy_60')), ]
#   simulation_quality.data <- pr.complete[which(pr.complete$criteria %in% c('simulation_quality_best', 'simulation_quality_worst')), ]
#   real_vs_worst_sim.data <- pr.complete[which(pr.complete$criteria %in% c('real_vs_simulated_worst', 'simulated_worst_vs_real')), ]
cutted <- performance[performance$criteria=='juged_bad',]
cutted <- rbind(cutted, performance[performance$criteria=='short_interactions',])
cutted <- rbind(cutted, performance[performance$criteria=='real',])
cutted <- rbind(cutted, performance[performance$criteria=='task_failed',])
cutted <- rbind(cutted, performance[performance$criteria=='word_accuracy_100',])
cutted <- rbind(cutted, performance[performance$criteria=='simulation_quality_best',])
cutted <- rbind(cutted, performance[performance$criteria=='real_vs_simulated_worst',])
# for rank order we need only one case for smoothing value, as the smoothing value is
# not used for rank order
cutted <- cutted[which(cutted$classifier_name == "rank order"), ]
# Set names for criteria
cutted$criteria_name <- 'NA'
cutted[which(cutted$criteria == 'juged_bad'),]$criteria_name <- 'user jugedment'
cutted[which(cutted$criteria == 'short_interactions'),]$criteria_name <- 'dialogue length'
cutted[which(cutted$criteria == 'real'),]$criteria_name <- 'real vs simulated'
cutted[which(cutted$criteria == 'task_failed'),]$criteria_name <- 'task success'
cutted[which(cutted$criteria == 'word_accuracy_100'),]$criteria_name <- 'word accuracy'
cutted[which(cutted$criteria == 'simulation_quality_best'),]$criteria_name <- 'real vs. simulated (good)'
cutted[which(cutted$criteria == 'real_vs_simulated_worst'),]$criteria_name <- 'real vs. simulated (bad)'
#   juged.data <- pr.complete[which(pr.complete$criteria %in% c('juged_bad', 'juged_good')), ]
#   interact_length.data <- pr.complete[which(pr.complete$criteria %in% c('short_interactions', 'long_interactions')), ]
#   real_simulated.data <- pr.complete[which(pr.complete$criteria %in% c('real', 'simulated')), ]
#   success.data <- pr.complete[which(pr.complete$criteria %in% c('task_failed', 'task_successful')), ]
#   word_accuracy.data <- pr.complete[which(pr.complete$criteria %in% c('word_accuracy_100', 'word_accuracy_60')), ]
#   simulation_quality.data <- pr.complete[which(pr.complete$criteria %in% c('simulation_quality_best', 'simulation_quality_worst')), ]
#   real_vs_worst_sim.data <- pr.complete[which(pr.complete$criteria %in% c('real_vs_simulated_worst', 'simulated_worst_vs_real')), ]
cutted <- performance[performance$criteria=='juged_bad',]
cutted <- rbind(cutted, performance[performance$criteria=='short_interactions',])
cutted <- rbind(cutted, performance[performance$criteria=='real',])
cutted <- rbind(cutted, performance[performance$criteria=='task_failed',])
cutted <- rbind(cutted, performance[performance$criteria=='word_accuracy_100',])
cutted <- rbind(cutted, performance[performance$criteria=='simulation_quality_best',])
cutted <- rbind(cutted, performance[performance$criteria=='real_vs_simulated_worst',])
# for rank order we need only one case for smoothing value, as the smoothing value is
# not used for rank order
#cutted <- cutted[which(cutted$classifier_name == "rank order"), ]
# Set names for criteria
cutted$criteria_name <- 'NA'
cutted[which(cutted$criteria == 'juged_bad'),]$criteria_name <- 'user jugedment'
cutted[which(cutted$criteria == 'short_interactions'),]$criteria_name <- 'dialogue length'
cutted[which(cutted$criteria == 'real'),]$criteria_name <- 'real vs simulated'
cutted[which(cutted$criteria == 'task_failed'),]$criteria_name <- 'task success'
cutted[which(cutted$criteria == 'word_accuracy_100'),]$criteria_name <- 'word accuracy'
cutted[which(cutted$criteria == 'simulation_quality_best'),]$criteria_name <- 'real vs. simulated (good)'
cutted[which(cutted$criteria == 'real_vs_simulated_worst'),]$criteria_name <- 'real vs. simulated (bad)'
#   juged.data <- pr.complete[which(pr.complete$criteria %in% c('juged_bad', 'juged_good')), ]
#   interact_length.data <- pr.complete[which(pr.complete$criteria %in% c('short_interactions', 'long_interactions')), ]
#   real_simulated.data <- pr.complete[which(pr.complete$criteria %in% c('real', 'simulated')), ]
#   success.data <- pr.complete[which(pr.complete$criteria %in% c('task_failed', 'task_successful')), ]
#   word_accuracy.data <- pr.complete[which(pr.complete$criteria %in% c('word_accuracy_100', 'word_accuracy_60')), ]
#   simulation_quality.data <- pr.complete[which(pr.complete$criteria %in% c('simulation_quality_best', 'simulation_quality_worst')), ]
#   real_vs_worst_sim.data <- pr.complete[which(pr.complete$criteria %in% c('real_vs_simulated_worst', 'simulated_worst_vs_real')), ]
cutted <- performance[performance$criteria=='juged_bad',]
cutted <- rbind(cutted, performance[performance$criteria=='short_interactions',])
cutted <- rbind(cutted, performance[performance$criteria=='real',])
cutted <- rbind(cutted, performance[performance$criteria=='task_failed',])
cutted <- rbind(cutted, performance[performance$criteria=='word_accuracy_100',])
cutted <- rbind(cutted, performance[performance$criteria=='simulation_quality_best',])
cutted <- rbind(cutted, performance[performance$criteria=='real_vs_simulated_worst',])
# for rank order we need only one case for smoothing value, as the smoothing value is
# not used for rank order
# select all rows where criteria name is not "rank order" and smoothing_value is not in (0.05, 0.25)
# That keeps all classifier != "rank order", and for "rank order" only those with smoothin_value == 0.5
cutted <- cutted[which( !(cutted$classifier_name == "rank order" & cutted$smoothing_value %in% c(0.05, 0.25)) ), ]
# Set names for criteria
cutted$criteria_name <- 'NA'
cutted[which(cutted$criteria == 'juged_bad'),]$criteria_name <- 'user jugedment'
cutted[which(cutted$criteria == 'short_interactions'),]$criteria_name <- 'dialogue length'
cutted[which(cutted$criteria == 'real'),]$criteria_name <- 'real vs simulated'
cutted[which(cutted$criteria == 'task_failed'),]$criteria_name <- 'task success'
cutted[which(cutted$criteria == 'word_accuracy_100'),]$criteria_name <- 'word accuracy'
cutted[which(cutted$criteria == 'simulation_quality_best'),]$criteria_name <- 'real vs. simulated (good)'
cutted[which(cutted$criteria == 'real_vs_simulated_worst'),]$criteria_name <- 'real vs. simulated (bad)'
nrow(cutted[which( !(cutted$classifier_name == "rank order" & cutted$smoothing_value %in% c(0.05, 0.25)) ), ])1120 + 224
1120 + 224
#   juged.data <- pr.complete[which(pr.complete$criteria %in% c('juged_bad', 'juged_good')), ]
#   interact_length.data <- pr.complete[which(pr.complete$criteria %in% c('short_interactions', 'long_interactions')), ]
#   real_simulated.data <- pr.complete[which(pr.complete$criteria %in% c('real', 'simulated')), ]
#   success.data <- pr.complete[which(pr.complete$criteria %in% c('task_failed', 'task_successful')), ]
#   word_accuracy.data <- pr.complete[which(pr.complete$criteria %in% c('word_accuracy_100', 'word_accuracy_60')), ]
#   simulation_quality.data <- pr.complete[which(pr.complete$criteria %in% c('simulation_quality_best', 'simulation_quality_worst')), ]
#   real_vs_worst_sim.data <- pr.complete[which(pr.complete$criteria %in% c('real_vs_simulated_worst', 'simulated_worst_vs_real')), ]
cutted <- performance[performance$criteria=='juged_bad',]
cutted <- rbind(cutted, performance[performance$criteria=='short_interactions',])
cutted <- rbind(cutted, performance[performance$criteria=='real',])
cutted <- rbind(cutted, performance[performance$criteria=='task_failed',])
cutted <- rbind(cutted, performance[performance$criteria=='word_accuracy_100',])
cutted <- rbind(cutted, performance[performance$criteria=='simulation_quality_best',])
cutted <- rbind(cutted, performance[performance$criteria=='real_vs_simulated_worst',])
# for rank order we need only one case for smoothing value, as the smoothing value is
# not used for rank order
# select all rows where criteria name is not "rank order" and smoothing_value is not in (0.05, 0.25)
# That keeps all classifier != "rank order", and for "rank order" only those with smoothin_value == 0.5
#cutted <- cutted[which( !(cutted$classifier_name == "rank order" & cutted$smoothing_value %in% c(0.05, 0.25)) ), ]
# Set names for criteria
cutted$criteria_name <- 'NA'
cutted[which(cutted$criteria == 'juged_bad'),]$criteria_name <- 'user jugedment'
cutted[which(cutted$criteria == 'short_interactions'),]$criteria_name <- 'dialogue length'
cutted[which(cutted$criteria == 'real'),]$criteria_name <- 'real vs simulated'
cutted[which(cutted$criteria == 'task_failed'),]$criteria_name <- 'task success'
cutted[which(cutted$criteria == 'word_accuracy_100'),]$criteria_name <- 'word accuracy'
cutted[which(cutted$criteria == 'simulation_quality_best'),]$criteria_name <- 'real vs. simulated (good)'
cutted[which(cutted$criteria == 'real_vs_simulated_worst'),]$criteria_name <- 'real vs. simulated (bad)'
measures <- unique(cutted$classifier_name)
for (i in 1:length(measures)) {
m <- measures[i]
print(
ggplot(cutted[which(cutted$classifier_name == m),], aes(x = auc)) +
geom_histogram(aes(y=..density..), colour="black", fill="white", binwidth=0.01) +
geom_density(alpha=.2, fill="#FF6666") +
geom_vline(aes(xintercept=mean(auc, na.rm=T)),
color="red", linetype="dashed", size=1) +
geom_vline(aes(xintercept=median(auc, na.rm=T)),
color="red", size=1) +
ggtitle(m)
)
}
measrue <- unique(cutted$classifier_name) # get classifier for filtering
data.frame(foo, bar)
data.frame(foo=, bar=)
data.frame(foo=character(0), bar=character(0))
library(plyr)
?dply
??dply
ddply(cutted)
ddply(cutted, c('criteria', 'classifier'))
ddply(cutted, c('criteria', 'classifier_name'))
ddply(cutted, c('criteria', 'classifier_name'), summarize)
ddply(cutted, c('criteria', 'classifier_name'), summarize, mean(auc))
ddply(cutted, c('criteria', 'classifier_name'), summarize, mean = mean(auc), min = min(auc))
ddply(cutted, c('criteria', 'classifier_name'), summarize, mean = mean(auc), median = median(auc), sd = sd(auc), min = min(auc), max(auc))
ddply(cutted, c('criteria', 'classifier_name'), summarize, mean = mean(auc), median = median(auc), sd = sd(auc), min = min(auc), max=max(auc))
foo <- ddply(cutted, c('criteria', 'classifier_name'), summarize, mean = mean(auc), median = median(auc), sd = sd(auc), min = min(auc), max=max(auc))
library(reshape)
library(reshape2)
dcast(foo, criteria~classifier_name)
dcast(foo, criteria ~ classifier_name, measure.var=c('min, 'max''))
?dcast
dcast(foo, criteria ~ classifier_name, value.var=c('min, 'max''))
dcast(foo, criteria ~ classifier_name, value.var=c('min', 'max'))
?recast
dcast(foo, criteria~classifier_name)
dcast(foo, criteria~classifier_name, id.vars=c('max'))
dcast(foo, criteria~classifier_name+max)
dcast(foo, criteria~max+classifier_name)
?merge
x <- dcast(foo, criteria~max+classifier_name)
y <- dcast(foo, criteria~max+classifier_name, value.var = 'min')
x
y
y <- dcast(foo, criteria~classifier_name, value.var = 'min')
x <- dcast(foo, criteria~classifier_name)
x
y
merge(x, y)
merge(x, y, by='criteria')
merge(x, y, by='criteria', sort=FALSE)
stats <- ddply(cutted, c('criteria', 'classifier_name'), summarize, mean = mean(auc), median = median(auc), sd = sd(auc), min = min(auc), max=max(auc))
stats
stats <- ddply(cutted, c('classifier_name', 'criteria'), summarize, mean = mean(auc), median = median(auc), sd = sd(auc), min = min(auc), max=max(auc))
stats
stats <- ddply(cutted, c('criteria', 'classifier_name'), summarize, mean = mean(auc), median = median(auc), sd = sd(auc), min = min(auc), max=max(auc))
stats
stats <- ddply(cutted, c('criteria_name', 'classifier_name'), summarize, mean = mean(auc), median = median(auc), sd = sd(auc), min = min(auc), max=max(auc))
stats <- ddply(cutted, c('criteria_name', 'classifier_name'), summarize, mean = mean(auc), median = median(auc), sd = sd(auc), min = min(auc), max=max(auc))
print(stats)
print(kable(stats))
stats <- ddply(cutted, c('criteria_name', 'classifier_name'), summarize, mean = mean(auc), median = median(auc), sd = sd(auc), min = min(auc), max=max(auc))
print(kable(stats, digits = 3))
